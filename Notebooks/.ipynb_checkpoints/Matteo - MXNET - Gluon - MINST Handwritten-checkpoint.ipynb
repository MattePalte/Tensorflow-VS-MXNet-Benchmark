{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MXNet version 1.5.1\n"
     ]
    }
   ],
   "source": [
    "# IMPORT LIBRARIES\n",
    "import mxnet\n",
    "from mxnet import nd, autograd\n",
    "print(\"MXNet version\", mxnet.__version__) # Matteo 1.5.1\n",
    "\n",
    "# Fixing the random seed\n",
    "mxnet.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORT LIBRARIES FOR TRAIN A NETWORK\n",
    "from mxnet import nd, gluon, init, autograd\n",
    "from mxnet.gluon import nn\n",
    "from mxnet.gluon.data.vision import datasets, transforms\n",
    "from IPython import display\n",
    "import matplotlib.pyplot as plt\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SETUP THE PROFILING\n",
    "from mxnet import profiler\n",
    "\n",
    "profiler.set_config(profile_all=True,\n",
    "                    aggregate_stats=True,\n",
    "                    continuous_dump=True,\n",
    "                    filename='profile_output_gluon.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional resources\n",
    "- Complete source file LeNet Digits https://gist.github.com/amohant4/f10e4f4f8a3f37f58e79be09a9ef8f87"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working - From Official Documentation\n",
    "https://mxnet.apache.org/api/python/docs/tutorials/packages/gluon/image/mnist.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GET DATASET AND SPLIT\n",
    "mnist = mxnet.test_utils.get_mnist()\n",
    "batch_size = 100\n",
    "train_data = mxnet.io.NDArrayIter(mnist['train_data'], mnist['train_label'], batch_size, shuffle=True)\n",
    "val_data = mxnet.io.NDArrayIter(mnist['test_data'], mnist['test_label'], batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nmxnet.gluon.nn.Flatten (no params)\\ndata: input tensor with arbitrary shape (N, x1, x2, …, xn)\\nout: 2D tensor with shape: (N, x1 cdot x2 cdot … cdot xn)\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# DEFINE THE NETWORK\n",
    "# INITIALIZE FASHION NET \n",
    "# IDENTICAL TO LeNet paper: http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf \n",
    "handwritten_net = nn.Sequential()\n",
    "handwritten_net.add(nn.Conv2D(channels=6, kernel_size=5, activation='relu'),\n",
    "        nn.MaxPool2D(pool_size=2, strides=2),\n",
    "        nn.Conv2D(channels=16, kernel_size=3, activation='relu'),\n",
    "        nn.MaxPool2D(pool_size=2, strides=2),\n",
    "        nn.Flatten(),\n",
    "        nn.Dense(120, activation=\"relu\"),\n",
    "        nn.Dense(84, activation=\"relu\"),\n",
    "        nn.Dense(10))\n",
    "# 2D convolution layer (e.g. spatial convolution over images).\n",
    "'''\n",
    "mxnet.gluon.nn.Conv2D(channels, kernel_size, activation=None, etc)\n",
    "channels (int) – \n",
    "    The dimensionality of the output space, \n",
    "    i.e. the number of output channels (filters) \n",
    "    in the convolution.\n",
    "kernel_size (int or tuple/list of 2 int) – \n",
    "    Specifies the dimensions of the convolution window.\n",
    "activation (str) – \n",
    "    Activation function to use. \n",
    "    See mxnet.ndarray.Activation\n",
    "    If you don’t specify anything, \n",
    "    no activation is applied (ie. “linear” activation: a(x) = x).\n",
    "'''\n",
    "\n",
    "# Max pooling operation for two dimensional (spatial) data.\n",
    "'''\n",
    "mxnet.gluon.nn.MaxPool2D(pool_size=(2, 2), strides=None, etc)\n",
    "pool_size (int or list/tuple of 2 ints,) – \n",
    "    Size of the max pooling windows.\n",
    "strides (int, list/tuple of 2 ints, or None.) – \n",
    "    Factor by which to downscale. \n",
    "    E.g. 2 will halve the input size. \n",
    "    If None, it will default to pool_size.\n",
    "'''\n",
    "\n",
    "# Flattens the input to two dimensional.\n",
    "'''\n",
    "mxnet.gluon.nn.Flatten (no params)\n",
    "data: input tensor with arbitrary shape (N, x1, x2, …, xn)\n",
    "out: 2D tensor with shape: (N, x1 cdot x2 cdot … cdot xn)\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Various ways for training in Gluon\n",
    "https://mxnet.apache.org/api/python/docs/tutorials/packages/gluon/training/index.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training acc at epoch 0: accuracy=0.758150\n",
      "training acc at epoch 1: accuracy=0.936833\n",
      "training acc at epoch 2: accuracy=0.956183\n",
      "training acc at epoch 3: accuracy=0.965467\n",
      "training acc at epoch 4: accuracy=0.970733\n"
     ]
    }
   ],
   "source": [
    "# TRAIN THE NETWORK WITH ACCURACY\n",
    "epoch = 5\n",
    "# CHECK IF GPUS ARE PRESENT\n",
    "gpus = mxnet.test_utils.list_gpus()\n",
    "ctx =  [mxnet.gpu()] if gpus else [mxnet.cpu(0), mxnet.cpu(1)]\n",
    "handwritten_net.initialize(mxnet.init.Xavier(), ctx=ctx, force_reinit=True)\n",
    "trainer = gluon.Trainer(handwritten_net.collect_params(), 'sgd', {'learning_rate': 0.02})\n",
    "# Use Accuracy as the evaluation metric.\n",
    "metric = mxnet.metric.Accuracy()\n",
    "softmax_cross_entropy_loss = gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "\n",
    "'''\n",
    "# Ask the profiler to start recording\n",
    "profiler.set_state('run')\n",
    "'''\n",
    "\n",
    "for i in range(epoch):\n",
    "    # Reset the train data iterator.\n",
    "    train_data.reset()\n",
    "    # Loop over the train data iterator.\n",
    "    for batch in train_data:\n",
    "        # Splits train data into multiple slices along batch_axis\n",
    "        # and copy each slice into a context.\n",
    "        data = gluon.utils.split_and_load(batch.data[0], ctx_list=ctx, batch_axis=0)\n",
    "        # Splits train labels into multiple slices along batch_axis\n",
    "        # and copy each slice into a context.\n",
    "        label = gluon.utils.split_and_load(batch.label[0], ctx_list=ctx, batch_axis=0)\n",
    "        outputs = []\n",
    "        # Inside training scope\n",
    "        with autograd.record():\n",
    "            for x, y in zip(data, label):\n",
    "                z = handwritten_net(x)\n",
    "                # Computes softmax cross entropy loss.\n",
    "                loss = softmax_cross_entropy_loss(z, y)\n",
    "                # Backpropogate the error for one iteration.\n",
    "                loss.backward()\n",
    "                outputs.append(z)\n",
    "        # Updates internal evaluation\n",
    "        metric.update(label, outputs)\n",
    "        # Make one step of parameter update. Trainer needs to know the\n",
    "        # batch size of data to normalize the gradient by 1/batch_size.\n",
    "        trainer.step(batch.data[0].shape[0])\n",
    "    # Gets the evaluation result.\n",
    "    name, acc = metric.get()\n",
    "    # Reset evaluation result to initial state.\n",
    "    metric.reset()\n",
    "    print('training acc at epoch %d: %s=%f'%(i, name, acc))\n",
    "'''\n",
    "# Make sure all operations have completed\n",
    "mxnet.nd.waitall()\n",
    "# Ask the profiler to stop recording\n",
    "profiler.set_state('stop')\n",
    "# Dump all results to log file before download\n",
    "profiler.dump()  \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation acc: accuracy=0.967800\n"
     ]
    }
   ],
   "source": [
    "# TEST THE NETWORK\n",
    "metric = mxnet.metric.Accuracy()\n",
    "# Reset the validation data iterator.\n",
    "val_data.reset()\n",
    "# Loop over the validation data iterator.\n",
    "for batch in val_data:\n",
    "    # Splits validation data into multiple slices along batch_axis\n",
    "    # and copy each slice into a context.\n",
    "    data = gluon.utils.split_and_load(batch.data[0], ctx_list=ctx, batch_axis=0)\n",
    "    # Splits validation label into multiple slices along batch_axis\n",
    "    # and copy each slice into a context.\n",
    "    label = gluon.utils.split_and_load(batch.label[0], ctx_list=ctx, batch_axis=0)\n",
    "    outputs = []\n",
    "    for x in data:\n",
    "        outputs.append(handwritten_net(x))\n",
    "    # Updates internal evaluation\n",
    "    metric.update(label, outputs)\n",
    "print('validation acc: %s=%f'%metric.get())\n",
    "assert metric.get()[1] > 0.94"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adaptation from Fashion MNIST tutorial - Problem with input shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GET THE TRAIN SET - HANDWRITTEN DIGITS\n",
    "# Apply a transformation to the dataset, normalize data from (0,255) -> (0,1)\n",
    "handwritten_train = datasets.MNIST(train=True, transform=lambda data, label: (data.astype(dtype='float32')/255, label))\n",
    "print(\"Nr samples: \", len(handwritten_train))\n",
    "X, y = handwritten_train[0]\n",
    "print('X shape: ', X.shape, 'X dtype', X.dtype, 'y shape:', y.shape)\n",
    "print( \"1 DATAPOINT consists in: 28 x 28 images - 10 outputs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VISUALIZE THE FIRST 10 DATAPOINTS\n",
    "# ONE FOR EACH LABEL\n",
    "\n",
    "text_labels = ['0', '1', '2', '3', '4',\n",
    "               '5', '6', '7', '8', '9']\n",
    "X, y = handwritten_train[0:10]\n",
    "# plot images\n",
    "display.set_matplotlib_formats('svg')\n",
    "# CREATE 10 FUGURES (X.shape[0] = 10)\n",
    "_, figs = plt.subplots(1, X.shape[0], figsize=(15, 15))\n",
    "# TAKING THE FIG OBJECT WE CAN MODIFY EVERYTHING\n",
    "for f,x,yi in zip(figs, X,y):\n",
    "    # 3D->2D by removing the last channel dim\n",
    "    f.imshow(x.reshape((28,28)).asnumpy())\n",
    "    ax = f.axes\n",
    "    ax.set_title(text_labels[int(yi)])\n",
    "    ax.title.set_fontsize(14)\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# CONVERT IMAGES FOR THE GLUON MODEL \n",
    "# IMAGES -> (channel, height, width)\n",
    "transformer = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(0.13, 0.31)])\n",
    "# 0.13 = real mean of the column\n",
    "# 0.31 = standard deviation\n",
    "fashion_train = fashion_train.transform_first(transformer)\n",
    "# JUST CREATED A LAZY TRANSFORMATION ... READY TO START\n",
    "print(fashion_train)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SELCT A BATCH SIZE (FOR GRADIENT COMPUTATION)\n",
    "batch_size = 256\n",
    "# PRODUCE A DATALOADER TO LOAD DATA\n",
    "# FEED THE DATASET (OR ITS LAZY TRANSFORMER)\n",
    "'''\n",
    "DataLoader\n",
    "Loads data from a dataset and returns mini-batches of data\n",
    "dataset : Dataset\n",
    "    Source dataset. Note that numpy and mxnet arrays can be directly used\n",
    "    as a Dataset.\n",
    "shuffle : bool\n",
    "        Whether to shuffle the samples.\n",
    "batch_size : int\n",
    "    Size of mini-batch.\n",
    "num_workers : int, default 0\n",
    "    The number of multiprocessing workers to use for data preprocessing.\n",
    "'''\n",
    "train_data = gluon.data.DataLoader(\n",
    "    handwritten_train, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "\n",
    "print(len(train_data))\n",
    "print(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The returned train_data is an <b>iterable object</b> that yields <b>batches of images</b> and <b>labels</b> pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GO THROUGH EACH PAIR \n",
    "# - DATAPOINT\n",
    "# - LABEL\n",
    "\n",
    "# PRINT ONE PAIR TO BE SURE\n",
    "\n",
    "for data, label in train_data:\n",
    "    print(data.shape, label.shape)\n",
    "    print(\"Pack of images (batch_size, channels, x_dim, y_dim) , Pack of labels\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATE VIDATION TEST (APPLY SAME TRANSFORMER)\n",
    "handwritten_valid = gluon.data.vision.MNIST(train=False, transform=lambda data, label: (data.astype(dtype='float32')/255, label)) \n",
    "# train = false is specific for default trainset in GLUON\n",
    "# normally we need to split by ourselves\n",
    "# GET ANOTHER DATA LOADER OBJECT (EXACTLY LIKE IN TRAIN)\n",
    "valid_data = gluon.data.DataLoader(\n",
    "    handwritten_valid,\n",
    "    batch_size=batch_size, num_workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Understand Convolution Low-Level\n",
    "http://machinelearninguru.com/computer_vision/basics/convolution/convolution_layer.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INITIALIZE FASHION NET \n",
    "# (IDENTICAL TO LeNet)\n",
    "handwritten_net = nn.Sequential()\n",
    "handwritten_net.add(nn.Conv2D(channels=6, kernel_size=5, activation='relu'),\n",
    "        nn.MaxPool2D(pool_size=2, strides=2),\n",
    "        nn.Conv2D(channels=16, kernel_size=3, activation='relu'),\n",
    "        nn.MaxPool2D(pool_size=2, strides=2),\n",
    "        nn.Flatten(),\n",
    "        nn.Dense(120, activation=\"relu\"),\n",
    "        nn.Dense(84, activation=\"relu\"),\n",
    "        nn.Dense(10))\n",
    "# 2D convolution layer (e.g. spatial convolution over images).\n",
    "'''\n",
    "mxnet.gluon.nn.Conv2D(channels, kernel_size, activation=None, etc)\n",
    "channels (int) – \n",
    "    The dimensionality of the output space, \n",
    "    i.e. the number of output channels (filters) \n",
    "    in the convolution.\n",
    "kernel_size (int or tuple/list of 2 int) – \n",
    "    Specifies the dimensions of the convolution window.\n",
    "activation (str) – \n",
    "    Activation function to use. \n",
    "    See mxnet.ndarray.Activation\n",
    "    If you don’t specify anything, \n",
    "    no activation is applied (ie. “linear” activation: a(x) = x).\n",
    "'''\n",
    "\n",
    "# Max pooling operation for two dimensional (spatial) data.\n",
    "'''\n",
    "mxnet.gluon.nn.MaxPool2D(pool_size=(2, 2), strides=None, etc)\n",
    "pool_size (int or list/tuple of 2 ints,) – \n",
    "    Size of the max pooling windows.\n",
    "strides (int, list/tuple of 2 ints, or None.) – \n",
    "    Factor by which to downscale. \n",
    "    E.g. 2 will halve the input size. \n",
    "    If None, it will default to pool_size.\n",
    "'''\n",
    "\n",
    "# Flattens the input to two dimensional.\n",
    "'''\n",
    "mxnet.gluon.nn.Flatten (no params)\n",
    "data: input tensor with arbitrary shape (N, x1, x2, …, xn)\n",
    "out: 2D tensor with shape: (N, x1 cdot x2 cdot … cdot xn)\n",
    "'''\n",
    "\n",
    "handwritten_net.initialize(init = init.Xavier())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "handwritten_net.summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xx = nd.random_uniform(shape=[28,28,28,28])\n",
    "out = handwritten_net(data)\n",
    "out\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "handwritten_net.initialize(init=init.Xavier())  # Initialize the parameters using Xavier initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEFINE A LOSS FUNCTION\n",
    "# CLASSIFICATION -> CROSSENTROPY\n",
    "# MULTICLASS -> SOFTMAX-CROSSENTROPY\n",
    "# (IF WOULD HAVE BEEN BINARY -> BINARY-CROSSENTROPY)\n",
    "softmax_cross_entropy = gluon.loss.SoftmaxCrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEFINE A TRAINER OBJECT\n",
    "# mxnet.gluon.Trainer(params, optimizer, optimizer_params=None, etc)\n",
    "# Applies an Optimizer on a set of Parameters. \n",
    "# Trainer should be used together with autograd.\n",
    "trainer = gluon.Trainer(handwritten_net.collect_params(), 'sgd', {'learning_rate': 0.1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ACCURACY METRIC (TYPICAL OF CLASSIFICATION)\n",
    "def acc(output, label):\n",
    "    # output: (batch, num_output) float32 ndarray\n",
    "    # label: (batch, ) int32 ndarray\n",
    "    return (output.argmax(axis=1) ==\n",
    "            label.astype('float32')).mean().asscalar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(10):\n",
    "    # RESET METRIC FOR EACH EPOCH\n",
    "    train_loss, train_acc, valid_acc = 0., 0., 0.\n",
    "    # TAKE THE TIME\n",
    "    tic = time.time()\n",
    "    # FOR EACH PAIRS OF \n",
    "    # - DATA (BATCH OF IMAGES - stack of n images where n = batch size)\n",
    "    # - LABELS (BATCH OF CLASS LABELS)\n",
    "    for data, label in train_data:\n",
    "        # forward + backward\n",
    "        with autograd.record():\n",
    "            # FEED INPUT AND GET OUTPUT - FORWARD PASS\n",
    "            output = handwritten_net(data)\n",
    "            # COMPUTE THE LOSS ON THIS BATCH\n",
    "            loss = softmax_cross_entropy(output, label)\n",
    "        # BACKWARD PASS - COMPUTE THE GRADIENTS WITH CHAIN RULES\n",
    "        loss.backward()\n",
    "        # update parameters\n",
    "        #\n",
    "        '''\n",
    "        step(batch_size, ignore_stale_grad=False)\n",
    "        Makes one step of parameter update. Should be called \n",
    "        after autograd.backward() and outside of record() scope.\n",
    "        - batch_size (int) – Batch size of data processed. \n",
    "            Gradient will be normalized by 1/batch_size. \n",
    "            Set this to 1 if you normalized loss manually \n",
    "            with loss = mean(loss).\n",
    "        - ignore_stale_grad (bool, optional, default=False) – \n",
    "            If true, ignores Parameters with stale gradient \n",
    "            (gradient that has not been updated by backward \n",
    "            after last step) and skip update.\n",
    "        '''\n",
    "        trainer.step(batch_size)\n",
    "        # calculate training metrics\n",
    "        train_loss += loss.mean().asscalar()\n",
    "        train_acc += acc(output, label)\n",
    "    # calculate validation accuracy\n",
    "    for data, label in valid_data:\n",
    "        valid_acc += acc(handwritten_net(data), label)\n",
    "    print(\"Epoch %d: loss %.3f, train acc %.3f, test acc %.3f, in %.1f sec\" % (\n",
    "            epoch, train_loss/len(train_data), train_acc/len(train_data),\n",
    "            valid_acc/len(valid_data), time.time()-tic))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OFFICIAL GUIDE - Saving and loading GLUON MODELS\n",
    "https://mxnet.apache.org/api/python/docs/tutorials/packages/gluon/blocks/save_load_params.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVAE MODEL\n",
    "# mxnet.gluon.Block \n",
    "# NB = also nn.Sequential() is a subclass of Block\n",
    "# method -> save_parameters(filename, deduplicate=False)\n",
    "'''\n",
    "Save parameters to file.\n",
    "Saved parameters can only be loaded with load_parameters. \n",
    "Note that this method only saves parameters, not model structure. \n",
    "If you want to save model structures, please use HybridBlock.export().\n",
    "'''\n",
    "# methid -> load_parameters(filename, ctx=None, allow_missing=False, \n",
    "# ignore_extra=False, cast_dtype=False, dtype_source='current')[source]\n",
    "fashion_net.save_parameters('fashion_net.params')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
