{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from time import time\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import keras.layers as layers\n",
    "from keras.models import Sequential\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.callbacks import TensorBoard\n",
    "# LOW LEVEL\n",
    "from keras.layers import Flatten\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_mnist(images_path: str, labels_path: str):\n",
    "    #mnist_path = \"data/mnist/\"\n",
    "    #images_path = mnist_path + images_path\n",
    "    print(images_path)\n",
    "    with gzip.open(labels_path, 'rb') as labelsFile:\n",
    "        labels = np.frombuffer(labelsFile.read(), dtype=np.uint8, offset=8)\n",
    "\n",
    "    with gzip.open(images_path,'rb') as imagesFile:\n",
    "        length = len(labels)\n",
    "        # Load flat 28x28 px images (784 px), and convert them to 28x28 px\n",
    "        features = np.frombuffer(imagesFile.read(), dtype=np.uint8, offset=16) \\\n",
    "                        .reshape(length, 784) \\\n",
    "                        .reshape(length, 28, 28, 1)\n",
    "        \n",
    "    return features, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train-images-idx3-ubyte.gz\n",
      "t10k-images-idx3-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "train = {}\n",
    "test = {}\n",
    "\n",
    "train['features'], train['labels'] = read_mnist('train-images-idx3-ubyte.gz', 'train-labels-idx1-ubyte.gz')\n",
    "test['features'], test['labels'] = read_mnist('t10k-images-idx3-ubyte.gz', 't10k-labels-idx1-ubyte.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of training images: 60000\n",
      "# of test images: 10000\n"
     ]
    }
   ],
   "source": [
    "print('# of training images:', train['features'].shape[0])\n",
    "print('# of test images:', test['features'].shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def display_image(position):\n",
    "    image = train['features'][position].squeeze()\n",
    "    plt.title('Example %d. Label: %d' % (position, train['labels'][position]))\n",
    "    plt.imshow(image, cmap=plt.cm.gray_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQMAAAEJCAYAAACQSkKtAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x27b5390a278>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_image(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>5923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>6742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>5958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>6131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>5842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>5421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>5918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>6265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>5851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>5949</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Label  Count\n",
       "0      0   5923\n",
       "1      1   6742\n",
       "2      2   5958\n",
       "3      3   6131\n",
       "4      4   5842\n",
       "5      5   5421\n",
       "6      6   5918\n",
       "7      7   6265\n",
       "8      8   5851\n",
       "9      9   5949"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels_count = np.unique(train['labels'], return_counts=True)\n",
    "dataframe_train_labels = pd.DataFrame({'Label':train_labels_count[0], 'Count':train_labels_count[1]})\n",
    "dataframe_train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "validation = {}\n",
    "train['features'], validation['features'], train['labels'], validation['labels'] = train_test_split(train['features'], train['labels'], test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of training images: 48000\n",
      "# of validation images: 12000\n"
     ]
    }
   ],
   "source": [
    "print('# of training images:', train['features'].shape[0])\n",
    "print('# of validation images:', validation['features'].shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated Image Shape: (32, 32, 1)\n"
     ]
    }
   ],
   "source": [
    "# Pad images with 0s\n",
    "train['features']      = np.pad(train['features'], ((0,0),(2,2),(2,2),(0,0)), 'constant')\n",
    "validation['features'] = np.pad(validation['features'], ((0,0),(2,2),(2,2),(0,0)), 'constant')\n",
    "test['features']       = np.pad(test['features'], ((0,0),(2,2),(2,2),(0,0)), 'constant')\n",
    "    \n",
    "print(\"Updated Image Shape: {}\".format(train['features'][0].shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HIGH LEVEL - KERAS\n",
    "https://colab.research.google.com/drive/1CVm50PGE4vhtB5I_a_yc4h5F-itKOVL9#scrollTo=1zQCBWvZCGon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential()\n",
    "\n",
    "model.add(layers.Conv2D(filters=6, kernel_size=(3, 3), activation='relu', input_shape=(32,32,1)))\n",
    "model.add(layers.AveragePooling2D())\n",
    "\n",
    "model.add(layers.Conv2D(filters=16, kernel_size=(3, 3), activation='relu'))\n",
    "model.add(layers.AveragePooling2D())\n",
    "\n",
    "model.add(layers.Flatten())\n",
    "\n",
    "model.add(layers.Dense(units=120, activation='relu'))\n",
    "\n",
    "model.add(layers.Dense(units=84, activation='relu'))\n",
    "\n",
    "model.add(layers.Dense(units=10, activation = 'softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 30, 30, 6)         60        \n",
      "_________________________________________________________________\n",
      "average_pooling2d_1 (Average (None, 15, 15, 6)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 13, 13, 16)        880       \n",
      "_________________________________________________________________\n",
      "average_pooling2d_2 (Average (None, 6, 6, 16)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 576)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 120)               69240     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 84)                10164     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                850       \n",
      "=================================================================\n",
      "Total params: 81,194\n",
      "Trainable params: 81,194\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=keras.losses.categorical_crossentropy, optimizer=keras.optimizers.Adam(), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "EPOCHS = 3\n",
    "BATCH_SIZE = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = train['features'], to_categorical(train['labels'])\n",
    "X_validation, y_validation = validation['features'], to_categorical(validation['labels'])\n",
    "\n",
    "train_generator = ImageDataGenerator().flow(X_train, y_train, batch_size=BATCH_SIZE)\n",
    "validation_generator = ImageDataGenerator().flow(X_validation, y_validation, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of training images: 48000\n",
      "# of validation images: 12000\n",
      "Epoch 1/3\n",
      "375/375 [==============================] - 9s 24ms/step - loss: 0.5162 - accuracy: 0.9024 - val_loss: 0.0828 - val_accuracy: 0.9677\n",
      "Epoch 2/3\n",
      "375/375 [==============================] - 9s 23ms/step - loss: 0.0898 - accuracy: 0.9729 - val_loss: 0.0211 - val_accuracy: 0.9752\n",
      "Epoch 3/3\n",
      "375/375 [==============================] - 9s 24ms/step - loss: 0.0622 - accuracy: 0.9804 - val_loss: 0.1097 - val_accuracy: 0.9821\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x27b56f56080>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('# of training images:', train['features'].shape[0])\n",
    "print('# of validation images:', validation['features'].shape[0])\n",
    "\n",
    "steps_per_epoch = X_train.shape[0]//BATCH_SIZE\n",
    "validation_steps = X_validation.shape[0]//BATCH_SIZE\n",
    "\n",
    "tensorboard = TensorBoard(log_dir=\"logs\\{}\".format(time()))\n",
    "model.fit_generator(train_generator, steps_per_epoch=steps_per_epoch, epochs=EPOCHS, \n",
    "                    validation_data=validation_generator, validation_steps=validation_steps, \n",
    "                    shuffle=True, callbacks=[tensorboard])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 1s 97us/step\n",
      "Test loss: 0.05825734651121311\n",
      "Test accuracy: 0.9819999933242798\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(test['features'], to_categorical(test['labels']))\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LOW LEVEL - TF.VARIABLE\n",
    "https://colab.research.google.com/drive/1kV3Jpxzup63GfJB1FGKxTSKd6Ek8J3sA#scrollTo=d669sWrHlP0g\n",
    "<br>Inspired by:<br>\n",
    "https://github.com/udacity/CarND-LeNet-Lab/blob/master/LeNet-Lab-Solution.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Architecture\n",
    "Convolutional #1 outputs 28x28x6\n",
    "\n",
    "Activation any activation function, we will relu\n",
    "Pooling #1 The output shape should be 14x14x6.\n",
    "\n",
    "Convolutional #2 outputs 10x10x16.\n",
    "\n",
    "Activation any activation function, we will relu\n",
    "Pooling #2 outputs 5x5x16.\n",
    "\n",
    "Flatten Flatten the output shape of the final pooling layer\n",
    "Fully Connected #1 outputs 120\n",
    "\n",
    "Activation any activation function, we will relu\n",
    "Fully Connected #2 outputs 84\n",
    "\n",
    "Activation any activation function, we will relu\n",
    "Fully Connected (Logits) #3 outpute 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_classes = 10\n",
    "\n",
    "def LeNet(x):    \n",
    "    # Arguments used for tf.truncated_normal, randomly defines variables for the weights and biases for each layer\n",
    "    mu = 0\n",
    "    sigma = 0.1    \n",
    "    \n",
    "    weights = {\n",
    "        # The shape of the filter weight is (height, width, input_depth, output_depth)\n",
    "        'conv1': tf.Variable(tf.truncated_normal(shape=(5, 5, 1, 6), mean = mu, stddev = sigma)),\n",
    "        'conv2': tf.Variable(tf.truncated_normal(shape=(5, 5, 6, 16), mean = mu, stddev = sigma)),\n",
    "        'fl1': tf.Variable(tf.truncated_normal(shape=(5 * 5 * 16, 120), mean = mu, stddev = sigma)),\n",
    "        'fl2': tf.Variable(tf.truncated_normal(shape=(120, 84), mean = mu, stddev = sigma)),\n",
    "        'out': tf.Variable(tf.truncated_normal(shape=(84, n_classes), mean = mu, stddev = sigma))\n",
    "    }\n",
    "\n",
    "    biases = {\n",
    "        # The shape of the filter bias is (output_depth,)\n",
    "        'conv1': tf.Variable(tf.zeros(6)),\n",
    "        'conv2': tf.Variable(tf.zeros(16)),\n",
    "        'fl1': tf.Variable(tf.zeros(120)),\n",
    "        'fl2': tf.Variable(tf.zeros(84)),\n",
    "        'out': tf.Variable(tf.zeros(n_classes))\n",
    "    }\n",
    "\n",
    "    # Layer 1: Convolutional. Input = 32x32x1. Output = 28x28x6.\n",
    "    conv1 = tf.nn.conv2d(x, weights['conv1'], strides=[1, 1, 1, 1], padding='VALID')\n",
    "    conv1 = tf.nn.bias_add(conv1, biases['conv1'])\n",
    "    # Activation.\n",
    "    conv1 = tf.nn.relu(conv1)\n",
    "    # Pooling. Input = 28x28x6. Output = 14x14x6.\n",
    "    conv1 = tf.nn.avg_pool(conv1, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='VALID')\n",
    "    \n",
    "    # Layer 2: Convolutional. Output = 10x10x16.\n",
    "    conv2 = tf.nn.conv2d(conv1, weights['conv2'], strides=[1, 1, 1, 1], padding='VALID')\n",
    "    conv2 = tf.nn.bias_add(conv2, biases['conv2'])\n",
    "    # Activation.\n",
    "    conv2 = tf.nn.relu(conv2)\n",
    "    # Pooling. Input = 10x10x16. Output = 5x5x16.\n",
    "    conv2 = tf.nn.avg_pool(conv2, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='VALID')\n",
    "\n",
    "    # Flatten. Input = 5x5x16. Output = 400.\n",
    "    fl0 = flatten(conv2)\n",
    "    \n",
    "    # Layer 3: Fully Connected. Input = 400. Output = 120.\n",
    "    fl1 = tf.add(tf.matmul(fl0, weights['fl1']), biases['fl1'])\n",
    "    # Activation.\n",
    "    fl1 = tf.nn.relu(fl1)\n",
    "    \n",
    "    # Layer 4: Fully Connected. Input = 120. Output = 84.\n",
    "    fl2 = tf.add(tf.matmul(fl1, weights['fl2']), biases['fl2'])\n",
    "    # Activation.\n",
    "    fl2 = tf.nn.relu(fl2)\n",
    "\n",
    "    # Layer 5: Fully Connected. Input = 84. Output = 10.\n",
    "    logits = tf.add(tf.matmul(fl2, weights['out']), biases['out'])\n",
    "                 \n",
    "    return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Features and Labels<br>\n",
    "x is a placeholder for a batch of input images.<br>\n",
    "y is a placeholder for a batch of output labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Migration\n",
    "https://www.tensorflow.org/guide/migrate#low-level_variables_operator_execution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LOW LEVEL GUIDE - 2.0\n",
    "https://www.juliabloggers.com/tensorflow-2-0-building-simple-classifier-using-low-level-apis/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Error converting shape to a TensorShape: Cannot convert <dtype: 'float32'> to Dimension.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\python36\\lib\\site-packages\\tensorflow_core\\python\\framework\\tensor_shape.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, dims)\u001b[0m\n\u001b[0;32m    769\u001b[0m       \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 770\u001b[1;33m         \u001b[0mdims_iter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0miter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdims\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    771\u001b[0m       \u001b[1;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'DType' object is not iterable",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\python36\\lib\\site-packages\\tensorflow_core\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mmake_shape\u001b[1;34m(v, arg_name)\u001b[0m\n\u001b[0;32m    205\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 206\u001b[1;33m     \u001b[0mshape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtensor_shape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_shape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    207\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python36\\lib\\site-packages\\tensorflow_core\\python\\framework\\tensor_shape.py\u001b[0m in \u001b[0;36mas_shape\u001b[1;34m(shape)\u001b[0m\n\u001b[0;32m   1215\u001b[0m   \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1216\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mTensorShape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1217\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python36\\lib\\site-packages\\tensorflow_core\\python\\framework\\tensor_shape.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, dims)\u001b[0m\n\u001b[0;32m    772\u001b[0m         \u001b[1;31m# Treat as a singleton dimension\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 773\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dims\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mas_dimension\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdims\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    774\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python36\\lib\\site-packages\\tensorflow_core\\python\\framework\\tensor_shape.py\u001b[0m in \u001b[0;36mas_dimension\u001b[1;34m(value)\u001b[0m\n\u001b[0;32m    717\u001b[0m   \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 718\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mDimension\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    719\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python36\\lib\\site-packages\\tensorflow_core\\python\\framework\\tensor_shape.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, value)\u001b[0m\n\u001b[0;32m    190\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDType\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 191\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Cannot convert %s to Dimension\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    192\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: Cannot convert <dtype: 'float32'> to Dimension",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-24-f3bbcca22dc1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackend\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mplaceholder\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mplaceholder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mplaceholder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mone_hot_y\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mone_hot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_classes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python36\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36msymbolic_fn_wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     73\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0m_SYMBOLIC_SCOPE\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mget_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 75\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     76\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     77\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python36\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36mplaceholder\u001b[1;34m(shape, ndim, dtype, sparse, name)\u001b[0m\n\u001b[0;32m    734\u001b[0m         \u001b[0mdtype\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfloatx\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    735\u001b[0m     x = tf_keras_backend.placeholder(\n\u001b[1;32m--> 736\u001b[1;33m         shape=shape, ndim=ndim, dtype=dtype, sparse=sparse, name=name)\n\u001b[0m\u001b[0;32m    737\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mshape\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    738\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mndim\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python36\\lib\\site-packages\\tensorflow_core\\python\\keras\\backend.py\u001b[0m in \u001b[0;36mplaceholder\u001b[1;34m(shape, ndim, dtype, sparse, name, ragged)\u001b[0m\n\u001b[0;32m   1050\u001b[0m           name=name)\n\u001b[0;32m   1051\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1052\u001b[1;33m       \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marray_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplaceholder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1053\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1054\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python36\\lib\\site-packages\\tensorflow_core\\python\\ops\\array_ops.py\u001b[0m in \u001b[0;36mplaceholder\u001b[1;34m(dtype, shape, name)\u001b[0m\n\u001b[0;32m   2628\u001b[0m                        \"eager execution.\")\n\u001b[0;32m   2629\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2630\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0mgen_array_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplaceholder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2631\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2632\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python36\\lib\\site-packages\\tensorflow_core\\python\\ops\\gen_array_ops.py\u001b[0m in \u001b[0;36mplaceholder\u001b[1;34m(dtype, shape, name)\u001b[0m\n\u001b[0;32m   6666\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mshape\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6667\u001b[0m     \u001b[0mshape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 6668\u001b[1;33m   \u001b[0mshape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_execute\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake_shape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"shape\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   6669\u001b[0m   _, _, _op = _op_def_lib._apply_op_helper(\n\u001b[0;32m   6670\u001b[0m         \"Placeholder\", dtype=dtype, shape=shape, name=name)\n",
      "\u001b[1;32mc:\\python36\\lib\\site-packages\\tensorflow_core\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mmake_shape\u001b[1;34m(v, arg_name)\u001b[0m\n\u001b[0;32m    206\u001b[0m     \u001b[0mshape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtensor_shape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_shape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    207\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 208\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Error converting %s to a TensorShape: %s.\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0marg_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    209\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    210\u001b[0m     raise ValueError(\"Error converting %s to a TensorShape: %s.\" % (arg_name,\n",
      "\u001b[1;31mTypeError\u001b[0m: Error converting shape to a TensorShape: Cannot convert <dtype: 'float32'> to Dimension."
     ]
    }
   ],
   "source": [
    "\n",
    "x = tf.placeholder(tf.float32, (None, 32, 32, 1))\n",
    "y = tf.placeholder(tf.int32, (None))\n",
    "one_hot_y = tf.one_hot(y, n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
