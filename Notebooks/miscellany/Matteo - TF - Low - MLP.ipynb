{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.juliabloggers.com/tensorflow-2-0-building-simple-classifier-using-low-level-apis/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# IMPORT LIBRARIES\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from enum import Enum\n",
    "from sklearn.datasets import load_iris\n",
    "from typing import Callable, Iterable, List, Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# DEFINE USEFUL CONSTANT\n",
    "\n",
    "class HyperParams(Enum):\n",
    "    ACTIVATION     = tf.nn.relu\n",
    "    BATCH_SIZE     = 5\n",
    "    EPOCHS         = 500\n",
    "    HIDDEN_NEURONS = 10\n",
    "    NORMALIZER     = tf.nn.softmax\n",
    "    OUTPUT_NEURONS = 3\n",
    "    OPTIMIZER      = tf.keras.optimizers.Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function tensorflow.python.ops.gen_nn_ops.relu>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HyperParams.ACTIVATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# LOAD DATA in X and Y\n",
    "iris = load_iris()\n",
    "xdat = iris.data\n",
    "ydat = iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150, 4)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xdat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# NICE (MAYBE USELESS) METHOD TO PARTITION DATA\n",
    "class Data:\n",
    "\n",
    "    def __init__(self, xdat: np.ndarray, ydat: np.ndarray, ratio: float = 0.3) -> Tuple:\n",
    "        self.xdat  = xdat\n",
    "        self.ydat  = ydat\n",
    "        self.ratio = ratio\n",
    "\n",
    "    def partition(self) -> None:\n",
    "        scnt = self.xdat.shape[0] / np.unique(self.ydat).shape[0]\n",
    "        ntst = int(self.xdat.shape[0] * self.ratio / (np.unique(self.ydat)).shape[0])\n",
    "        idx  = np.random.choice(np.arange(0, self.ydat.shape[0] / np.unique(self.ydat).shape[0], dtype = int), ntst, replace = False)\n",
    "        for i in np.arange(1, np.unique(self.ydat).shape[0]):\n",
    "            idx = np.concatenate((idx, np.random.choice(np.arange((scnt * i), scnt * (i + 1), dtype = int), ntst, replace = False)))\n",
    "\n",
    "        self.xtrn = self.xdat[np.where(~np.in1d(np.arange(0, self.ydat.shape[0]), idx))[0], :]\n",
    "        self.ytrn = self.ydat[np.where(~np.in1d(np.arange(0, self.ydat.shape[0]), idx))[0]]\n",
    "        self.xtst = self.xdat[idx, :]\n",
    "        self.ytst = self.ydat[idx]\n",
    "\n",
    "    def to_tensor(self, depth: int = 3) -> None:\n",
    "        self.xtrn = tf.convert_to_tensor(self.xtrn, dtype = np.float32) \n",
    "        self.xtst = tf.convert_to_tensor(self.xtst, dtype = np.float32)\n",
    "        self.ytrn = tf.convert_to_tensor(tf.one_hot(self.ytrn, depth = depth))\n",
    "        self.ytst = tf.convert_to_tensor(tf.one_hot(self.ytst, depth = depth))\n",
    "    \n",
    "    def batch(self, num: int = 16) -> None:\n",
    "        try:\n",
    "            size = self.xtrn.shape[0] / num\n",
    "            if self.xtrn.shape[0] % num != 0:\n",
    "                sizes = [tf.floor(size).numpy().astype(int) for i in range(num)] + [self.xtrn.shape[0] % num]\n",
    "            else:\n",
    "                sizes = [tf.floor(size).numpy().astype(int) for i in range(num)]\n",
    "\n",
    "            self.xtrn_batches = tf.split(self.xtrn, num_or_size_splits = sizes, axis = 0)\n",
    "            self.ytrn_batches = tf.split(self.ytrn, num_or_size_splits = sizes, axis = 0)\n",
    "\n",
    "            num = int(self.xtst.shape[0] / sizes[0])\n",
    "            if self.xtst.shape[0] % sizes[0] != 0:\n",
    "                sizes = [sizes[i] for i in range(num)] + [self.xtst.shape[0] % sizes[0]]\n",
    "            else:\n",
    "                sizes = [sizes[i] for i in range(num)]\n",
    "\n",
    "            self.xtst_batches = tf.split(self.xtst, num_or_size_splits = sizes, axis = 0)\n",
    "            self.ytst_batches = tf.split(self.ytst, num_or_size_splits = sizes, axis = 0)\n",
    "        except:\n",
    "            self.xtrn_batches = [self.xtrn]\n",
    "            self.ytrn_batches = [self.ytrn]\n",
    "            self.xtst_batches = [self.xtst]\n",
    "            self.ytst_batches = [self.ytst]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# DATA CLEANING AND DATA PROCESSING\n",
    "data = Data(xdat, ydat)\n",
    "data.partition()\n",
    "data.to_tensor()\n",
    "data.batch(HyperParams.BATCH_SIZE.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define a layer y = σ(x ⋅ W + b)\n",
    "class Dense:\n",
    "\n",
    "    def __init__(self, i: int, o: int, f: Callable[[tf.Tensor], tf.Tensor], initializer: Callable = tf.random.normal) -> None:\n",
    "        self.w = tf.Variable(initializer([i, o]))\n",
    "        self.b = tf.Variable(initializer([o]))\n",
    "        self.f = f\n",
    "\n",
    "    def __call__(self, x: tf.Tensor) -> tf.Tensor:\n",
    "        if callable(self.f):\n",
    "            return self.f(tf.add(tf.matmul(x, self.w), self.b))\n",
    "        else:\n",
    "            return tf.add(tf.matmul(x, self.w), self.b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=79, shape=(1, 2), dtype=float32, numpy=array([[5.5676966, 0.       ]], dtype=float32)>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# PERFORM FEED FORWARD\n",
    "layer = Dense(4, 2, tf.nn.relu)\n",
    "# GET THE OUTPUT OF THIS NEWLY CREATED LAYER\n",
    "# IF FED WITH ONE INPUT DATAPOINT FROM THE TRAINSET\n",
    "# REMEMBER! EAGER EXECUTION! WE GET THE OUTPUT RIGHT NOW\n",
    "layer(data.xtrn[1:2, :])\n",
    "#> tf.Tensor([[12.937485  0.      ]], shape=(1, 2), dtype=float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=115, shape=(4, 2), dtype=float32, numpy=\n",
       "array([[5.567697 , 0.       ],\n",
       "       [6.1614122, 0.       ],\n",
       "       [6.9433584, 0.       ],\n",
       "       [5.930968 , 0.       ]], dtype=float32)>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# FEED WITH 4 INPUT DATAPOINTS...\n",
    "# AND ...\n",
    "# YOU'LL GET 4 OUTPUT DAATAPOINTS!!!\n",
    "# MAGIC\n",
    "layer(data.xtrn[1:5, :])\n",
    "#> tf.Tensor(\n",
    "#> [[12.937484  0.      ]\n",
    "#>  [12.557415  0.      ]\n",
    "#>  [13.761768  0.      ]\n",
    "#>  [14.996015  0.      ]], shape=(4, 2), dtype=float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# MORE POWER -> WE WANT A CHAIN OF LAYERS NOW!\n",
    "class Chain:\n",
    "\n",
    "    def __init__(self, layers: List[Iterable[Dense]]) -> None:\n",
    "        self.layers = layers\n",
    "    \n",
    "    def __call__(self, x: tf.Tensor) -> tf.Tensor:\n",
    "        self.out = x; self.params = []\n",
    "        for l in self.layers:\n",
    "            self.out = l(self.out)\n",
    "            self.params.append([l.w, l.b])\n",
    "        \n",
    "        self.params = [j for i in self.params for j in i]\n",
    "        return self.out\n",
    "\n",
    "    def backward(self, inputs: tf.Tensor, targets: tf.Tensor) -> None:\n",
    "        # HERE WE LEARN :D -> THE MAGIC IS HERE\n",
    "        grads = self.grad(inputs, targets)\n",
    "        self.optimize(grads, 0.001)\n",
    "    \n",
    "    def loss(self, preds: tf.Tensor, targets: tf.Tensor) -> tf.Tensor:\n",
    "        return tf.reduce_mean(\n",
    "            tf.keras.losses.categorical_crossentropy(\n",
    "                targets, preds\n",
    "            )\n",
    "        )\n",
    "        \n",
    "    def grad(self, inputs: tf.Tensor, targets: tf.Tensor) -> List:\n",
    "        with tf.GradientTape() as g:\n",
    "            error = self.loss(self(inputs), targets)\n",
    "        \n",
    "        return g.gradient(error, self.params)\n",
    "\n",
    "    def optimize(self, grads: List[tf.Tensor], rate: float) -> None:\n",
    "        opt = HyperParams.OPTIMIZER.value(learning_rate = rate)\n",
    "        opt.apply_gradients(zip(grads, self.params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# CREATE A MODEL...\n",
    "#YES .. OUR MODEL IS A TRIVIAL CHAIN OF DENSE LAYERS...\n",
    "# TWO LAYERS PRECISELY\n",
    "model = Chain([\n",
    "    Dense(data.xtrn.shape[1], HyperParams.HIDDEN_NEURONS.value, HyperParams.ACTIVATION),\n",
    "    Dense(HyperParams.HIDDEN_NEURONS.value, HyperParams.OUTPUT_NEURONS.value, HyperParams.NORMALIZER)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=253, shape=(1, 3), dtype=float32, numpy=array([[9.5460939e-01, 9.1180876e-09, 4.5390658e-02]], dtype=float32)>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TRY TO FEED SOMETHING...\n",
    "# REMEMBER! EAGER EXECUTION! WE GET THE OUTPUT RIGHT NOW\n",
    "model(data.xtrn[1:2, :])\n",
    "#> tf.Tensor([[2.8523763e-16 1.8208168e-15 1.0000000e+00]], shape=(1, 3), dtype=float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=267, shape=(4, 3), dtype=float32, numpy=\n",
       "array([[9.5460939e-01, 9.1180707e-09, 4.5390647e-02],\n",
       "       [9.7212243e-01, 1.5073410e-09, 2.7877590e-02],\n",
       "       [9.6991414e-01, 5.6262484e-10, 3.0085811e-02],\n",
       "       [9.6266270e-01, 6.0332490e-09, 3.7337299e-02]], dtype=float32)>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(data.xtrn[1:5, :])\n",
    "#> tf.Tensor(\n",
    "#> [[2.8523763e-16 1.8208168e-15 1.0000000e+00]\n",
    "#>  [4.9846957e-16 8.1282060e-16 1.0000000e+00]\n",
    "#>  [6.2472026e-16 1.2082151e-15 1.0000000e+00]\n",
    "#>  [1.8308374e-17 2.8908239e-17 1.0000000e+00]], shape=(4, 3), dtype=float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# CUSTOM DEFINITION OF MODEL ACCURACY...\n",
    "# THE ACCURACY IS WHAT WE WANT.\n",
    "# FUCK THE STANDARDIZATION OF KERAS\n",
    "# WE ARE DIFFERENT\n",
    "def accuracy(y, yhat):\n",
    "    # BUT IN THE END LET?S IMPLEMENT A STANDARD \n",
    "    # ACCURACY XD\n",
    "    # LET'S BE CREATIVE ANOTHER DAY\n",
    "    j = 0; correct = []\n",
    "    for i in tf.argmax(y, 1):\n",
    "        if i == tf.argmax(yhat[j]):\n",
    "            correct.append(1)\n",
    "        \n",
    "        j += 1\n",
    "    \n",
    "    num = tf.cast(tf.reduce_sum(correct), dtype = tf.float32)\n",
    "    den = tf.cast(y.shape[0], dtype = tf.float32)\n",
    "    return num / den"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\python36\\lib\\site-packages\\tensorflow_core\\python\\ops\\math_grad.py:1394: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Epoch:    0 \t Training Error: 4.9966 \t Testing Error: 3.6393 \t Accuracy Training: 0.6667 \t Accuracy Testing: 0.7619\n",
      "Epoch:   20 \t Training Error: 3.4939 \t Testing Error: 2.5276 \t Accuracy Training: 0.6667 \t Accuracy Testing: 0.7619\n",
      "Epoch:   40 \t Training Error: 2.3230 \t Testing Error: 1.6882 \t Accuracy Training: 0.6667 \t Accuracy Testing: 0.7619\n",
      "Epoch:   60 \t Training Error: 1.4973 \t Testing Error: 1.0984 \t Accuracy Training: 0.6667 \t Accuracy Testing: 0.7619\n",
      "Epoch:   80 \t Training Error: 0.8956 \t Testing Error: 0.7481 \t Accuracy Training: 0.6571 \t Accuracy Testing: 0.7619\n",
      "Epoch:  100 \t Training Error: 0.7920 \t Testing Error: 0.7211 \t Accuracy Training: 0.6571 \t Accuracy Testing: 0.7619\n",
      "Epoch:  120 \t Training Error: 0.7674 \t Testing Error: 0.6982 \t Accuracy Training: 0.6571 \t Accuracy Testing: 0.7619\n",
      "Epoch:  140 \t Training Error: 0.7366 \t Testing Error: 0.6754 \t Accuracy Training: 0.6571 \t Accuracy Testing: 0.7619\n",
      "Epoch:  160 \t Training Error: 0.7131 \t Testing Error: 0.6533 \t Accuracy Training: 0.6571 \t Accuracy Testing: 0.7619\n",
      "Epoch:  180 \t Training Error: 0.6836 \t Testing Error: 0.6315 \t Accuracy Training: 0.6667 \t Accuracy Testing: 0.7619\n",
      "Epoch:  200 \t Training Error: 0.6597 \t Testing Error: 0.6103 \t Accuracy Training: 0.6667 \t Accuracy Testing: 0.7619\n",
      "Epoch:  220 \t Training Error: 0.6379 \t Testing Error: 0.5898 \t Accuracy Training: 0.6667 \t Accuracy Testing: 0.7619\n",
      "Epoch:  240 \t Training Error: 0.6152 \t Testing Error: 0.5698 \t Accuracy Training: 0.6667 \t Accuracy Testing: 0.7619\n",
      "Epoch:  260 \t Training Error: 0.5887 \t Testing Error: 0.5504 \t Accuracy Training: 0.6667 \t Accuracy Testing: 0.7619\n",
      "Epoch:  280 \t Training Error: 0.5674 \t Testing Error: 0.5315 \t Accuracy Training: 0.6667 \t Accuracy Testing: 0.7619\n",
      "Epoch:  300 \t Training Error: 0.5407 \t Testing Error: 0.5034 \t Accuracy Training: 0.6667 \t Accuracy Testing: 0.7619\n",
      "Epoch:  320 \t Training Error: 0.5100 \t Testing Error: 0.4742 \t Accuracy Training: 0.6667 \t Accuracy Testing: 0.7619\n",
      "Epoch:  340 \t Training Error: 0.4812 \t Testing Error: 0.4483 \t Accuracy Training: 0.6667 \t Accuracy Testing: 0.7619\n",
      "Epoch:  360 \t Training Error: 0.4591 \t Testing Error: 0.4286 \t Accuracy Training: 0.6667 \t Accuracy Testing: 0.7619\n",
      "Epoch:  380 \t Training Error: 0.4409 \t Testing Error: 0.4121 \t Accuracy Training: 0.6762 \t Accuracy Testing: 0.7619\n",
      "Epoch:  400 \t Training Error: 0.4265 \t Testing Error: 0.3980 \t Accuracy Training: 0.6952 \t Accuracy Testing: 0.7619\n",
      "Epoch:  420 \t Training Error: 0.4141 \t Testing Error: 0.3862 \t Accuracy Training: 0.7143 \t Accuracy Testing: 0.7778\n",
      "Epoch:  440 \t Training Error: 0.4056 \t Testing Error: 0.3770 \t Accuracy Training: 0.7524 \t Accuracy Testing: 0.8095\n",
      "Epoch:  460 \t Training Error: 0.3982 \t Testing Error: 0.3705 \t Accuracy Training: 0.7714 \t Accuracy Testing: 0.8095\n",
      "Epoch:  480 \t Training Error: 0.3935 \t Testing Error: 0.3656 \t Accuracy Training: 0.7905 \t Accuracy Testing: 0.8095\n"
     ]
    }
   ],
   "source": [
    "# TRAINING PROCEDURE\n",
    "epoch_trn_loss = []\n",
    "epoch_tst_loss = []\n",
    "epoch_trn_accy = []\n",
    "epoch_tst_accy = []\n",
    "'''\n",
    "As you can see, we have three loops, two of which are \n",
    "inner-loops for the minibatches on both training and \n",
    "testing datasets. Needless to say, the minibatches used \n",
    "in the testing dataset above are not really necessary, \n",
    "since we can have a single batch for validation. \n",
    "However, we have them for purpose of comparing the \n",
    "performance of the optimization algorithm on both single \n",
    "batch and three minibatches. \n",
    "'''\n",
    "for j in range(HyperParams.EPOCHS.value):\n",
    "    # TRIVIAL LISTS WHERE TO COLLECT THE LOSS FOR EACH DATAPOINT\n",
    "    trn_loss = []; trn_accy = []\n",
    "    for i in range(len(data.xtrn_batches)):\n",
    "        # ALL THE MAGIC HAPPEN IN BACKWARD METHOD.... :D\n",
    "        model.backward(data.xtrn_batches[i], data.ytrn_batches[i])\n",
    "        ypred = model(data.xtrn_batches[i])\n",
    "        trn_loss.append(model.loss(ypred, data.ytrn_batches[i]))\n",
    "        trn_accy.append(accuracy(data.ytrn_batches[i], ypred))\n",
    "\n",
    "    trn_err = tf.reduce_mean(trn_loss).numpy()\n",
    "    trn_acy = tf.reduce_mean(trn_accy).numpy()\n",
    "\n",
    "    tst_loss = []; tst_accy = []\n",
    "    for i in range(len(data.xtst_batches)):\n",
    "        ypred = model(data.xtst_batches[i])\n",
    "        tst_loss.append(model.loss(ypred, data.ytst_batches[i]))\n",
    "        tst_accy.append(accuracy(data.ytst_batches[i], ypred))\n",
    "    \n",
    "    # GET THE OVERALL LOSS AND ACCURACY \n",
    "    tst_err = tf.reduce_mean(tst_loss).numpy()\n",
    "    tst_acy = tf.reduce_mean(tst_accy).numpy()\n",
    "    \n",
    "    epoch_trn_loss.append(trn_err)\n",
    "    epoch_tst_loss.append(tst_err)\n",
    "    epoch_trn_accy.append(trn_acy)\n",
    "    epoch_tst_accy.append(tst_acy)\n",
    "    \n",
    "    if j % 20 == 0:\n",
    "        print(\"Epoch: {0:4d} \\t Training Error: {1:.4f} \\t Testing Error: {2:.4f} \\t Accuracy Training: {3:.4f} \\t Accuracy Testing: {4:.4f}\".format(j, trn_err, tst_err, trn_acy, tst_acy))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Finally, the following tabularizes the statistics \n",
    "# we obtained from model estimation.\n",
    "df = pd.DataFrame({\n",
    "    \"trn_loss\" : epoch_trn_loss,\n",
    "    \"trn_accy\" : epoch_trn_accy,\n",
    "    \"tst_loss\" : epoch_tst_loss,\n",
    "    \"tst_accy\" : epoch_tst_accy\n",
    "})\n",
    "\n",
    "df.to_csv(\"../tf2_output_normal_initializer_batch_size_\" + str(HyperParams.BATCH_SIZE.value) + \".csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
